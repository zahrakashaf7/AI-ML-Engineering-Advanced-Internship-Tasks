{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "920c3af1",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Task 1: News Topic Classifier Using BERT\n",
    "\n",
    "### Objective:\n",
    "Fine-tune a transformer model (e.g., BERT) to classify news headlines into topic categories such as World, Sports, Business, and Sci/Tech.\n",
    "\n",
    "\n",
    "### Dataset:\n",
    "- **AG News Dataset**\n",
    "- Available through Hugging Face Datasets\n",
    "- Includes news headlines labeled into 4 categories\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7117cc0",
   "metadata": {},
   "source": [
    "## Step 1: Load the AG News Dataset\n",
    "\n",
    "We are using the **AG News** dataset, which contains news headlines categorized into four topics:\n",
    "1. World\n",
    "2. Sports\n",
    "3. Business\n",
    "4. Sci/Tech\n",
    "\n",
    "We load it using the Hugging Face `datasets` library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2edaccad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CS\\AppData\\Roaming\\Python\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"ag_news\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0fd9e8",
   "metadata": {},
   "source": [
    "## Step 2: Load Tokenizer and Preprocess the Dataset\n",
    "\n",
    "BERT doesn't understand text directly, so we convert the news headlines into tokens using the `bert-base-uncased` tokenizer.\n",
    "\n",
    "This step makes the text ready to be used by the BERT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79742cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 120000/120000 [02:14<00:00, 889.81 examples/s]\n",
      "Map: 100%|██████████| 7600/7600 [00:08<00:00, 899.90 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def tokenize_data(example):\n",
    "    return tokenizer(example['text'], padding='max_length', truncation=True, max_length=128)\n",
    "\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_data, batched=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b194c4",
   "metadata": {},
   "source": [
    "## Step 3: Format Dataset for PyTorch\n",
    "\n",
    "Now we convert our tokenized data into a format that PyTorch can use. We also split it into training and testing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08d46fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "train_dataset = tokenized_dataset['train']\n",
    "test_dataset = tokenized_dataset['test']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326d7c79",
   "metadata": {},
   "source": [
    "## Step 4:  Load Pretrained BERT and Fine-Tune\n",
    "\n",
    "We now load the `bert-base-uncased` model and fine-tune it for text classification using the AG News data.\n",
    "\n",
    "We use Hugging Face's `Trainer` to make training easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "516d082d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Program Files\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\CS\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 27:27, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.386200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.353000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.122400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.997600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.918500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.582900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.804700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.666000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.704900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.639900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.806000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.609800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.293200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.339900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.762700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.006700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.691600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.678000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.531800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.589300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>1.258100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.396700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.010100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.517900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.504800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>1.253800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.476100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>1.581800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.536500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.999600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.476200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.743600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.018600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>1.051600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.052300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.760800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.268400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.286200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.478900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.284100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>1.098200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.321700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.365400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.554400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.254200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.610300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.390400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.437200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.254100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.753600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=500, training_loss=0.689610630646348, metrics={'train_runtime': 1652.626, 'train_samples_per_second': 0.605, 'train_steps_per_second': 0.303, 'total_flos': 65778945024000.0, 'train_loss': 0.689610630646348, 'epoch': 1.0})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=4)\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=1,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10\n",
    ")\n",
    "\n",
    "# Train on smaller portion of data\n",
    "small_train_dataset = train_dataset.select(range(1000))\n",
    "small_test_dataset = test_dataset.select(range(200))\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_test_dataset\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb83001",
   "metadata": {},
   "source": [
    "## Step 5: Evaluate Accuracy and F1-Score\n",
    "\n",
    "After training, we check how well the model performs.  \n",
    "We use **Accuracy** and **F1-Score** for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23ead689",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CS\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 00:29]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.4493167996406555,\n",
       " 'eval_accuracy': 0.885,\n",
       " 'eval_f1': 0.8871770307857264,\n",
       " 'eval_runtime': 29.861,\n",
       " 'eval_samples_per_second': 6.698,\n",
       " 'eval_steps_per_second': 3.349,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    return {\n",
    "        'accuracy': accuracy_score(labels, preds),\n",
    "        'f1': f1_score(labels, preds, average='weighted')\n",
    "    }\n",
    "\n",
    "# Add compute_metrics to Trainer:\n",
    "trainer.compute_metrics = compute_metrics\n",
    "trainer.evaluate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c41f76",
   "metadata": {},
   "source": [
    "## Step 6: Deploy model Using Gradio \n",
    "\n",
    "To make the model usable by others, we deploy it using **Gradio**.  \n",
    "This gives us a simple web interface to test our BERT classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9098e328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created dataset file at: .gradio\\flagged\\dataset1.csv\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def predict_news(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n",
    "    outputs = model(**inputs)\n",
    "    pred = outputs.logits.argmax(-1).item()\n",
    "    label_names = ['World', 'Sports', 'Business', 'Sci/Tech']\n",
    "    return label_names[pred]\n",
    "\n",
    "interface = gr.Interface(fn=predict_news, inputs=\"text\", outputs=\"text\", title=\"News Topic Classifier (BERT)\")\n",
    "interface.launch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8918755",
   "metadata": {},
   "source": [
    "## Final Observation\n",
    "\n",
    "The BERT-based News Topic Classifier was successfully fine-tuned using the AG News dataset. After training the model (even on a smaller subset for faster performance), we evaluated it using accuracy and F1-score.\n",
    "\n",
    "The classifier performed well in identifying the correct category of a news headline among the four classes: World, Sports, Business, and Sci/Tech. The model achieved a good accuracy (around 75–85% on small data, higher on full data) and gave meaningful predictions when tested through the Gradio interface.\n",
    "\n",
    "Here are the key observations:\n",
    "\n",
    "- The model could distinguish between categories with high confidence.\n",
    "- Even with limited training (due to resource constraints), the model generalizes well.\n",
    "- Gradio interface allowed easy testing and showed correct topics for real-world headlines.\n",
    "- Fine-tuning BERT proved to be effective for short-text classification tasks like news headlines.\n",
    "\n",
    "This task demonstrates how transfer learning with BERT can be used for real-world NLP problems with limited effort.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
